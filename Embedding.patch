--- /usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py
+++ /usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py
@@ -69,18 +69,9 @@
     __constants__ = ['num_embeddings', 'embedding_dim', 'padding_idx', 'max_norm',
                      'norm_type', 'scale_grad_by_freq', 'sparse']
 
-    num_embeddings: int
-    embedding_dim: int
-    padding_idx: int
-    max_norm: float
-    norm_type: float
-    scale_grad_by_freq: bool
-    weight: Tensor
-    sparse: bool
-
-    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None,
-                 max_norm: Optional[float] = None, norm_type: float = 2., scale_grad_by_freq: bool = False,
-                 sparse: bool = False, _weight: Optional[Tensor] = None) -> None:
+    def __init__(self, num_embeddings, embedding_dim, padding_idx=None,
+                 max_norm=None, norm_type=2., scale_grad_by_freq=False,
+                 sparse=False, _weight=None):
         super(Embedding, self).__init__()
         self.num_embeddings = num_embeddings
         self.embedding_dim = embedding_dim
@@ -103,18 +94,18 @@
             self.weight = Parameter(_weight)
         self.sparse = sparse
 
-    def reset_parameters(self) -> None:
+    def reset_parameters(self):
         init.normal_(self.weight)
         if self.padding_idx is not None:
             with torch.no_grad():
                 self.weight[self.padding_idx].fill_(0)
 
-    def forward(self, input: Tensor) -> Tensor:
+    def forward(self, input):
         return F.embedding(
             input, self.weight, self.padding_idx, self.max_norm,
             self.norm_type, self.scale_grad_by_freq, self.sparse)
 
-    def extra_repr(self) -> str:
+    def extra_repr(self):
         s = '{num_embeddings}, {embedding_dim}'
         if self.padding_idx is not None:
             s += ', padding_idx={padding_idx}'